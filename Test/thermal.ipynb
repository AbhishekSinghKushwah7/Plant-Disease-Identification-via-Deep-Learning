{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIvpaZyBYeopOOA9pE6O0f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhishekSinghKushwah7/Plant-Disease-Identification-via-Deep-Learning/blob/main/thermal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment 2**"
      ],
      "metadata": {
        "id": "1NCnOMfnnKGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqdZGtyOsRxE",
        "outputId": "9b7e40d3-1c28-4b61-8dcf-a461b61ce97e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kanEUk54lv8o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_directories(output_dir, splits, classes):\n",
        "    for split in splits:\n",
        "        for cls in classes:\n",
        "            os.makedirs(os.path.join(output_dir, split, cls), exist_ok=True)\n",
        "\n",
        "def list_all_images(source_dir, classes):\n",
        "    all_images = []\n",
        "    for cls in classes:\n",
        "        class_dir = os.path.join(source_dir, cls)\n",
        "        images = [os.path.join(class_dir, img) for img in os.listdir(class_dir) if img.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        all_images.extend([(img, cls) for img in images])\n",
        "    return all_images\n",
        "\n",
        "def split_dataset(all_images):\n",
        "    train_val_images, test_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
        "    train_images, val_images = train_test_split(train_val_images, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "    return train_images, val_images, test_images\n",
        "\n",
        "def copy_images(images, output_dir, split):\n",
        "    for img_path, cls in images:\n",
        "        dest_dir = os.path.join(output_dir, split, cls)\n",
        "        shutil.copy(img_path, dest_dir)\n",
        "\n",
        "def prepare_data(source_dir, output_dir, classes):\n",
        "    splits = ['train', 'validation', 'test']\n",
        "    create_directories(output_dir, splits, classes)\n",
        "    all_images = list_all_images(source_dir, classes)\n",
        "    train_images, val_images, test_images = split_dataset(all_images)\n",
        "    copy_images(train_images, output_dir, 'train')\n",
        "    copy_images(val_images, output_dir, 'validation')\n",
        "    copy_images(test_images, output_dir, 'test')\n",
        "    return len(train_images), len(val_images), len(test_images)\n",
        "\n",
        "def build_resnet_model(input_shape, num_classes):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=input_shape))\n",
        "\n",
        "    # Unfreeze more layers for fine-tuning\n",
        "    for layer in base_model.layers[-30:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "    # Add custom layers on top of the base model\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu', kernel_initializer='he_normal', kernel_regularizer=l2(0.01))(x)  # Added L2 regularization and weight init\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu', kernel_initializer='he_normal')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-2), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def plot_history(history, test_accuracy=None):\n",
        "    plt.figure(figsize=(10, 4))  # Reduced size for side-by-side plots\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    if test_accuracy is not None:\n",
        "        plt.axhline(y=test_accuracy, color='r', linestyle='--', label='Test Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training, Validation, and Test Accuracy')\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "wWZuTLAVnD8g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = \"/content/drive/MyDrive/Colab Notebooks/thermal images UL\"\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/thermal images UL\"\n",
        "classes = ['healthy', 'Blast', 'BLB', 'hispa', 'leaf folder', 'leaf spot']"
      ],
      "metadata": {
        "id": "yFjsC08_ouBE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "train_count, val_count, test_count = prepare_data(source_dir, output_dir, classes)\n",
        "print(f\"Training set: {train_count} images\")\n",
        "print(f\"Validation set: {val_count} images\")\n",
        "print(f\"Test set: {test_count} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxQNYZWx0Gyl",
        "outputId": "6a6b95fb-baa3-4b03-9c62-488190eb9adf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 381 images\n",
            "Validation set: 127 images\n",
            "Test set: 128 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=20,  # Reduced\n",
        "    width_shift_range=0.1,  # Reduced\n",
        "    height_shift_range=0.1,  # Reduced\n",
        "    shear_range=0.1,  # Reduced\n",
        "    zoom_range=0.1,  # Reduced\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    os.path.join(output_dir, 'train'),\n",
        "    target_size=(224, 224),  # Resize images to 224x224\n",
        "    batch_size=128,  # Increased batch size\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    os.path.join(output_dir, 'validation'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = val_datagen.flow_from_directory(\n",
        "    os.path.join(output_dir, 'test'),\n",
        "    target_size=(224, 224),\n",
        "    batch_size=128,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdjrCZEX95kR",
        "outputId": "62f755ca-bad0-40ce-bfed-5c30df55270b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 540 images belonging to 8 classes.\n",
            "Found 227 images belonging to 8 classes.\n",
            "Found 232 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Model\n",
        "model = build_resnet_model(input_shape=(224, 224, 3), num_classes=train_generator.num_classes)\n"
      ],
      "metadata": {
        "id": "1Qm0kNxc-Gma"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=10,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJ6EByr--PZk",
        "outputId": "32433879-4637-4cd1-fb24-6d149c45abec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: ['keras_tensor']\n",
            "Received: inputs=Tensor(shape=(None, 224, 224, 3))\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Model\n",
        "train_accuracy = history.history['accuracy'][-1]\n",
        "val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(f\"Training Accuracy: {train_accuracy}\")\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Testing Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "b69S7vgQeNow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Training/Validation Accuracy and Loss\n",
        "plot_history(history, test_accuracy=test_accuracy)"
      ],
      "metadata": {
        "id": "xI8UYVHjeLFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Model\n",
        "model.save('resnet_image_classification_model.h5')\n",
        "print(\"Model saved as 'resnet_image_classification_model.h5'\")"
      ],
      "metadata": {
        "id": "uTEkWy1UZe2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
